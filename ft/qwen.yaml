# 模型 & 方法
model_name_or_path: /home/h3c/luozhongze/LLaMA-Factory/Qwen2.5-7B-Instruct
stage: sft
do_train: true
finetuning_type: lora
lora_target: ["W_pack", "o_proj", "k_proj", "v_proj", "q_proj", "gate_proj", "up_proj", "down_proj"]
lora_rank: 16
lora_alpha: 32

# 数据集
dataset: G,V,X
template: qwen
cutoff_len: 2048
max_samples: 6000
overwrite_cache: true
preprocessing_num_workers: 16

# 输出 & 日志
output_dir: saves/Qwen2.5-7B-Instruct/lora/qwen2.5-7B/2
logging_dir: saves/Qwen2.5-7B-Instruct/lora/qwen2.5-7B/2/logs
logging_steps: 10
save_steps: 500
plot_loss: true
overwrite_output_dir: true

# 训练设置
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 5.0e-5
num_train_epochs: 2
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
flash_attn: auto  # 如果模型支持
ddp_timeout: 180000000

# 验证
val_size: 0.1
per_device_eval_batch_size: 2
eval_strategy: steps
eval_steps: 100

load_best_model_at_end: true
metric_for_best_model: eval_loss
greater_is_better: false
